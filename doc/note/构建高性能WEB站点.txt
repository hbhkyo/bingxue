第一章 绪论
1.等待的真相：数据在网络的传输时间 服务器处理请求并生成相应数据的时间 浏览器本地计算和渲染的时间
2.找出瓶颈在哪里
3.增加带宽
4.减少页面中的http请求
5.加快服务器脚本计算速度
6.使用动态内容缓存
7.使用数据缓存
8.将动态内容静态化
9.更换web服务器软件
10.页面组件分离
11.合理部署服务器
12.使用负载均衡
13.优化数据库
14.考虑可扩展性
15.减少视觉等待

第二章 数据的网络传输
1.带宽 单位 bit/s 单位时间的比特数 从数字通信的角度讲带宽就是数据在网络中传输的速度，即bit/s
（1）数据发送：先是程序写入数据到进程的内存中，再调用系统接口复制数据到内核缓冲区（一个有限的队列）。再由网卡缓冲区来复制数据。
在这些复制过程中一般采取两端内部总线宽度来复制数据。如32位总线，即32位的比特信息。网卡缓冲区再发送到线路中（需要进行字节到位的转换），释放缓冲区。
电信号传播速度是2.3*10*8m/s  光信号是3.0*10*8m/s 但是光是全反射传播（类似正玄波），所以传播长度大于光纤长度。实际速度为2.0*10*m/s。
（2）100M网卡，即100Mbit/s.也就是一秒钟能发100Mbit的数据。
跟下面两个决定因素原因有关：
*数据发送装置的信号传输频率，以及数据接受装置的接受能力和线路对传输频率的支持程度。
*数据传播介质的并行度，即宽度。等价计算机总线宽度的概念。比如多条光纤组成光缆，在一个横截面传输多个信号。就像32位总线的计算机，可以同一时刻传输32位数据。
要想提高计算机总线宽度，可以提高总线宽度和总线频率。比如使用64位总线或频率更高的处理器。
（3）限制带宽
数据传输过程：服务器发送到交换机缓冲队列，交换机发送到路由转发队列。再发送到用户机器。
带宽限制是由交换机接受装置，限制服务器的发送速度。
共享和独享带宽。独享10M带宽，就是10Mbit/s 换算成字节就是1250.0KBytes/s 独享的是路由器的带宽，而不是交换机的带宽。因为交换机都是各个端口独享的带宽互不影响。
2.响应时间 数据从服务器开始发送直到完全到达用户PC的这段时间。
（1）下载速度：bytes/s 单位时间从服务器到达用户PC的数据量。
下载速度=数据量字节数/响应时间
响应时间=发送时间（数据量比特数/带宽）+传播时间（传播距离/传播速度）+处理时间
单位：bit比特缩写为b  Byte字节缩写为B 1B=8b  1KB=2*10B=1024B  1Kb=10*3b=1000b 

第三章 服务器并发处理能力
1.吞吐率：单位时间内服务器处理的请求数。reqs/s 
并发用户数 总请求数 请求资源描述 
*吞吐率=总请求数/总处理时间
*用户平均请求等待时间=总处理时间/（总请求数/并发数）
*服务器平均请求处理时间=总处理时间/总请求数 正是吞吐率的倒数，也等于用户平均请求等待时间/并发数
2.CPU并发计算
多执行流一般实现是进程 

第四章 动态内容缓存
缓冲，为匹配各硬件之间速度不匹配而引发的问题。如内存先将数据写入磁盘缓冲，再由磁盘缓冲写入磁盘。
缓存，为减少计算开销，将计算结果保存在内存或磁盘中。
1.页面缓存（HTML）
磁盘缓存 效率低
内存缓存 依赖系统内存不好扩展
缓存服务器 可以实现分布式缓存扩展
2.页面缓存支持局部无缓存。
3.静态化内容 吞吐率会受带宽的影响。用CMS管理静态内容。
（1）数据更新时生成静态内容。
（2）定时生成静态内容。
（3）并且支持局部更新。如SSI（服务器端包含）实现各个局部页面的独立更新。SSI有性能影响，会影响吞吐率，但是能更好的管理静态内容。需要把握平衡点。

第五章 动态脚本加速
除非静态化，否则完全跳过动态脚本执行是不可能的。使用缓存时任然需要执行动态脚本。
动态脚本缓存机制对动态脚本加速。能提升CPU和内存计算。但是提升IO有限。通过分析计算动态脚本代码执行瓶颈进行优化。

第六章 浏览器缓存
GET才会启用本地缓存，POST不会。
1.浏览器缓存目录
2.缓存协商
（1）last-modified 协商机制。缺点：有时内容没有更新而时间更新了，或者是分布式部署不容易控制最后修改时间一样。
（2）ETag机制
浏览器器缓存只是针对所属用户，而服务器缓存时针对所有用户。浏览器缓存最大好处是节省带宽。
expires 机制刷新后无效 ，可能会和服务器有时间差，需要配合cache-control使用

第七章 WEB服务器缓存。
存储在磁盘或内存中。

第八章 反向代理缓存
缓存在反向代理服务器上面。

第九章 web组件分离
1.域名分离：拥有不同的域名。浏览器并发数会提高。
2.动态内容分离：足够快的CPU，足够大的内存，多进程，与数据库保持高速连接，可靠的数据中心。
3.静态内容：足够的带宽，单进程，高速磁盘，NIO，RAID分区。
4.图片，样式表，JS脚本： 缓存在浏览器。

第十章 分布式缓存
1.memcached 
（1）缓存的是二进制数据，对象的数据会缓存，对象的函数不会缓存。
（2）写操作缓存，支持原子加法，应尽量减少锁竞争。
（3）监控状态，缓存过期时间，命中率，IO流量。
（4）易扩展。

第十一章 数据库性能优化
1.正确的使用索引
2.SQL的解释查询
3.组合索引遵循最左前缀原则。order by 对HASH索引无效，BTree有效。
4.使用索引后索引文件会大，有时需要维护手动索引。
5.行锁和表锁
6.事物配置
7.反范式设计
8.放弃关系型数据库，增加key value数据库

第十二章 web负载均衡
避免单点故障，做到高可用性
1.HTTP重定向
如镜像下载  
（1）RR-round robin依次轮询重定向： HTTP是无状态的，维护变量需要开销。能保证任务均衡。但是绝对均衡是没有的。如下载未完成，用户固定访问单台机器。
（2）随机重定向： 开销小，任务随机
2.DNS负载均衡
（1）智能解析
（2）故障转移
动态DNS也会存在一点延迟。无法拿到HTTP请求的上下文，策略开发存在局限性。
3.反向代理负载均衡
工作在HTTP层面
（1）按权重分配任务
（2）调度器的并发处理能力 当后端服务器处理时间短，吞吐率高时。调度器性能下降。
（3）健康探测
（4）粘带会话 通过IP实现均衡或者cookies机制实现均衡，因为可能是一个IP公网后面用户
避免这种粘带会话设计，用分布式session或分布式缓存。
4.IP负载均衡
工作在HTTP层面下层，支持更多协议 FTP SMTP DNS
（1） LVS-NAT 网关 工作在传输层，对IP和端口进行修改转发，也称四层负载均衡
当后端服务器吞吐率达到一定高度后，反向代理服务器吞吐率将达到极限，而LVS-NAT负载均衡则会达到一个新的高度的吞吐率。几乎是反向代理服务器的两倍以上。
这是因为在内核中进行请求转发的低开销。但是大量响应数据会受到带宽影响。使NAT成为瓶颈，需要与DNS-RR配合LVS-NAT集群使用。
（2） LVS-DR  直接路由 direct route 工作在数据链路层（第二层），对路由MAC地址修改转发。
响应数据可以绕过调度器，直接发送给用户端。需要购买很多合法IP地址。
（3） LVS-TUN IP隧道
和DR都适合请求和响应不对称的web服务器。可以有效地提高集群扩展能力。如果需要部署在不同的IDC，根据就近访问原则转移请求。IP隧道更合适。

第十三章 共享文件系统
NFS 依赖单点的解决方案，适用站点内的资源共享，不适合IO密集型的文件共享。

第十四章 内容分发和同步
1.复制
主动分发：SSH SCP SFTP
被动同步： rsync hash tree
2.多级分发 WebDAV
区别：
*文件分发需要一定的应用逻辑，需要编写扩展代码来控制文件传输。而文件同步非常透明，易部署。
*文件分发更好控制触发条件，更加灵活，容易实现多级分发。
*文件同步是天然的异步复制操作。不阻塞web服务运行。而异步分发需要额外支持，比如异步计算等。
3.反向代理

第十五章 分布式文件系统
hadoop 存储节点，web服务器通过访问追踪器，再访问对应的节点
追踪器：维护节点信息，与客户端和节点通信，控制文件复制策略，实现存储故障转移，提高可用性。
追踪器负者维护抽象层面的存储空间，而节点负者维护物理层面的存储空间。
分布式文件系统优点：
可以组建大量廉价的服务器海量存储系统。通过冗余复制保证高可用性。
拥有好的扩展性，增加追踪器和存储节点都很容易。实现文件副本的负载均衡。通过扩展保证性能。

第十六章 数据库扩展
1.复制和分离
主从复制，读写分离。写在主服务器，读在从服务器。
2.使用数据库代理服务器
3.垂直分区扩展
针对写操作频繁的站点，将不同的数据放到不同的服务器上面做分区。不涉及到join查询。同样分区可以主从复制。读写分离。
4.水平分区扩展
当同一个数据表写操作达到极限，需要分区分表。
分表：单台数据库做法，时间分表，ID分表等算法将数据放在不同表。
分区：多台数据库做法，hash算法，范围算法，映射关系等分区。放在不同数据库上面。
分区后也支持数据库反向代理服务器。

第十七章 分布式计算
1.分布式消息队列 client server - job server - worker server 发邮件场景
2.并行计算 Map/Reduce 如何分解任务，冗余计算，汇总

第十八章 性能监控
1.实时监控
2.监控代理
3.系统监控
4.服务监控
5.响应时间监控
